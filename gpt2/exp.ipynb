!pip install transformers
from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained("gpt2") # size -> 124M
state_dict_model = model.state_dict()

for k,v in state_dict_model.items():
    print(k,v.shape)
transformer.wte.weight torch.Size([50257, 768]) -> 50257 : number of tokens 
-> 768 : length of embedding 


transformer.wpe.weight torch.Size([1024, 768])
->1024 : number of positions the token can tend to 
-> is the positional embedding

import matplotlib.pyplot as plt

## plotting the values

plt.imshow(state_dict_model['transformer.wpe.weight'])
## visualising individual columns of the weights

plt.plot(state_dict_model['transformer.wpe.weight'][:,120])
plt.plot(state_dict_model['transformer.wpe.weight'][:,150])
with open('input.txt','r') as f:
    text = f.read()

data = text[:1000]

print(data[:100])
import tiktoken

encoder = tiktoken.get_encoding('gpt2')
tokens = encoder.encode(data)
print(tokens[:24])
import torch
buffer = torch.tensor(tokens[:24 + 1]) # +1 for getting the correct next token for 24th token

x = buffer[:-1].view(4,6) # all tokens except 24th ( 0 based indexing)
y = buffer[1:].view(4,6) # all next tokens for first to 23rd token
print(y)
print(x)
